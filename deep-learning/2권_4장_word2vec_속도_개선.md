# word2vec 속도 개선

## Summary

1. CBOW 모델을 사용할 때 말뭉치가 커질수록 계산량이 매우 커진다. 속도를 개선하기 위해 Embedding, 이진 분류, 네거티브 샘플링 등의 기법을 이용한다.
2. 단어 분산 표현을 통해 자연어를 벡터로 변환할 수 있다. 따라서 이를 일반적인 머신러닝 기법에 적용할 수 있게 된다.
3. 단어 분산 표현의 우수성을 평가하는 방법에는 단어의 유사성과 유추 문제를 활용한 평가가 있다.

</br>

</br>

</br>

</br>

</br>

## word2vec 개선 (1)

### CBOW 모델의 문제점

![IMG_0435](https://user-images.githubusercontent.com/59877415/253595782-03f64e9a-f4f2-4175-afe7-199bb50d26b5.jpg)

말뭉치에 포함된 어휘의 수가 많아지면 계산량이 커진다.

1. 입력층의 원핫 표현과 가중치 행렬 Win의 곱계산 : Embedding이라는 새로운 계층을 통해서 해결
2. 은닉층과 가중치 행렬 Wout의 곱 및 Softmax 계층의 계산 : 네거티브 샘플링이라는 새로운 손실 함수로 해결

</br>

### Embedding 계층

![IMG_0436](https://user-images.githubusercontent.com/59877415/253595799-ff1dc1a8-a7bd-473b-ae94-db0c69632e4c.jpg)

입력층의 원핫 표현과 가중치 행렬 Win의 곱계산에서 결과적으로 수행하는 일은 행렬의 특정 행을 추출하는 것이다. 따라서 원 핫 표현으로의 변환과 MatMul 계층의 곱 계산은 필요하지 않다. 가중치 매개변수로부터 특정 행을 추출하는 계층을 Emnedding 계층이라고 부른다.

</br>

### Embedding 계층 구현

![IMG_0437](https://user-images.githubusercontent.com/59877415/253595802-6ef231c8-cd70-41ec-91fb-adcdd82cd0cb.jpg)

가중치 W가 2차원 넘파이 배열이므로 가중치의 특정행을 추출할 때에는 W[3], W[5]처럼 원하는 행을 명시하면 된다.

</br>

![IMG_0438](https://user-images.githubusercontent.com/59877415/253595806-bf4ebbab-eea2-4f9c-9c19-76035692c74e.jpg)

가중치 기울기 dW를 꺼낸 다음, dW의 형상은 유지한 채로 원소들을 0으로 덮어쓴다. 이후 앞 층에서 전해진 기울이 dout을 idx번째 행에 할당한다.

</br>

![IMG_0439](https://user-images.githubusercontent.com/59877415/253595888-21792895-e5d9-44ea-b8fd-0a71e2782326.jpg)

idx의 원소가 중복될 때 문제가 발생한다. (You say goodbye and I say hello. 에서 say와 같은 경우) 먼저 쓰여진 값이 덮여쓰여진다는 점이다.

</br>

![IMG_0440](https://user-images.githubusercontent.com/59877415/253595898-43a7ab7e-2a1e-4243-81f6-d65d1cac9ad2.jpg)

이러한 중복 문제를 해결하려면 할당이 아닌 더하기를 해주어야 한다.

</br>

</br>

</br>

</br>

</br>

## word2vec 개선 (2)

### 은닉층 이후 계산의 문제점

![IMG_0435](https://user-images.githubusercontent.com/59877415/253595782-03f64e9a-f4f2-4175-afe7-199bb50d26b5.jpg)

은닉층 이후에서 계산이 오래 걸리는 곳은 두 부분이다.

1. 은닉층의 뉴런과 가중치 행렬(Wout)의 곱 (시간이 오래 걸리고 메모리도 많이 필요)
2. Softmax 계층의 계산 (계산량이 크다)

</br>

### 다중 분류에서 이진 분류로

다중 분류를 이진 분류로 근사하는 것이 네커티브 샘플링의 중요한 포인트이다.

- 다중 분류 : 맥락이 you와 goodbye일 때, 타깃 단어는 무엇입니까?
- 이진 분류 : 맥락이 you와 goodbye일 때, 타깃 단어는 say입니까?

</br>

![IMG_0442](https://user-images.githubusercontent.com/59877415/253595913-71326a1e-e725-46ab-a427-d9cfc31c1382.jpg)

출력층의 뉴런 중 필요한 것은 하나뿐이다.

</br>

![IMG_0443](https://user-images.githubusercontent.com/59877415/253595915-a2d7955b-6d54-4ca4-a61c-20479bf8ea85.jpg)

say에 해당하는 열벡터와 은닉층의 뉴런의 내적만을 계산하면 된다.

</br>

### 시그모이드 함수와 교차 엔트로피 오차

이진 분류 문제를 풀기 위해서는 점수에 시그모이드 함수를 적용해 확률로 변환한다. (시그모이드 함수의 출력을 확률로 해석할 수 있다.) 손실을 구할 때는 손실 함수로 '교차 엔트로피 오차'를 사용한다.



![IMG_0444](https://user-images.githubusercontent.com/59877415/253595918-b6256b6b-d4c4-4791-ab55-1f71904a2806.jpg)

역전파의 y-t값은 신경망의 출력과 정답 레이블의 차이이다. 오차가 클수록 크게 학습하고 오차가 작을수록 작게 학습한다.

</br>

### 네거티브 샘플링

![IMG_0445](https://user-images.githubusercontent.com/59877415/253595958-2e78642d-e0f2-41c7-9da7-9850e6d8a248.jpg)

지금까지는 긍정적인 예시에 대해서 학습했고 부정적인 예를 입력하면 어떤 결과가 나올지는 확실하지 않다. 그러나 우리의 목표는 긍정적인 예에 대해서는 Sigmoid 계층의 출력을 1에 가깝게 만들고 부정적인 예에 대해서는 Sigmoid 계층의 출력을 0에 가깝게 만드는 것이다.

</br>

모든 부정적 예를 대상으로 이진 분류를 시킨다면 다중 분류를 했을 때와 다를 바가 없다. 따라서 부정적인 예를 몇개 선택하여 사용한다. 이를 네거티브 샘플링이라고 한다.

</br>

![IMG_0446](https://user-images.githubusercontent.com/59877415/253595965-c4cadbd0-1789-468c-b305-3eff48f11c35.jpg)

각 데이터의 손실을 모두 더하여 최종 손실을 계산한다.

</br>

부정적인 예를 어떻게 샘플링해야 할까? 말뭉치의 통계 데이터를 기초로 샘플링한다. 자주 등장하는 단어를 많이 추출하고 드물게 등장하는 단어를 적게 추출하는 것이다. (흔한 단어를 잘 처리하는 편이 좋은 결과로 이어진다.)

</br>

### CBOW 모델 평가

![IMG_0447](https://user-images.githubusercontent.com/59877415/253595968-d0fc7252-cf56-40f1-8249-9c7fb40b85e4.jpg)

CBOW 모델로 획득된 단어의 분산 표현은 제법 괜찮은 특성을 지닌다.

</br>

</br>

</br>

</br>

</br>

## word2vec 남은 주제

### 전이 학습

word2vec으로 얻은 단어의 분산 표현이 자연어 처리 분야에서 중요한 이유는 전이 학습에 있다.

> 전이 학습 : 한 분야에서 배운 지식을 다른 분야에도 적용하는 기법

</br>

- 단어 분산 표현의 장점
  - 단어를 고정 길이 벡터로 변환해준다.
  - 문장도 고정 길이 벡터로 변환할 수 있다.
    - 문장을 고정 길이 벡터로 변환하는 방법은 활발하게 연구되고 있는데, 가장 간단한 방법은 문장의 각 단어를 분산 표현으로 변환하고 순서를 고려하지 않고 합을 구하는 방법이다. 이를 bag-of-words라고 한다.
  - 자연어를 벡터로 변환할 수 있다면 일반적인 머신러닝 기법(신경망, SVM)에 적용할 수 있다.
- 메일 자동 분류 시스템 등에 사용될 수 있다.

</br>

### 단어 벡터 평가 방법

1. 단어의 분산 표현의 우수성 평가 척도 1 : 단어의 유사성

- 사람이 작성한 단어 유사도를 검증 세트를 사용해 평가한다.
- cat과 car의 유사도는 2점, cat과 animal의 유사도는 8점

2. 단어의 분산 표현의 우수성 평가 척도 2 : 유추 문제를 활용한 평가

- king : queen = man : ? 와 같은 유추 문제를 풀게 하고 정답률로 우수성을 측정한다.

</br>

![IMG_0448](https://user-images.githubusercontent.com/59877415/253595970-5340f026-7f4c-4869-be24-8e81eb05095a.jpg)

3. 결과

- 모델에 따라 정확도가 다르다.
- 일반적으로 말뭉치가 클수록 결과가 좋다.
- 단어 벡터의 차원 수는 적당한 크기가 좋다.
