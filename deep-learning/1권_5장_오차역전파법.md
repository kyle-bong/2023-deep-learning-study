# 5. 오차역전파법

## ✨등장 배경

- 수치 미분의 단점: 계산 시간이 오래 걸린다. 오차역전파법(backpropagation)을 통해 가중치 매개변수의 기울기를 효율적으로 계산할 수 있다.

## ✨리마인드

- 손실함수
    - 모델의 예측과 정답의 차이를 계산하는 함수
    - 손실함수의 값을 최소화하여야 모델의 성능이 좋아진다.
    - 즉, 손실함수의 값을 가장 작게 만들도록 가중치 매개변수를 업데이트해야 한다.
- 경사하강법
    - 현재 위치에서 기울어진 방향으로 가중치 매개변수의 값을 업데이트하며 손실함수의 값을 최소화하는 최적의 가중치 매개변수 값을 찾는 방법.

## ✨오늘 발표 내용 요약

- 계산 그래프, 연쇄법칙, 역전파
- 오차역전파법 구현

---

## 5.1. 계산 그래프

- 계산 그래프란?
	![1](https://github.com/shinhee-rebecca/2023-deep-learning-study/assets/42907231/1824fd55-8d95-4b65-8df4-d66722e9548a)
    
    - 계산 과정을 그래프로 나타낸 것
    - 그래프는 복수의 node와 edge로 표현됨
    - 순전파: 왼쪽에서부터 오른쪽으로 계산, 역전파: 오른쪽에서부터 왼쪽으로 계산
- 계산 그래프의 이점
    - 국소적 계산을 통해, 각 노드에서는 단순한 계산에만 집중하여 문제를 단순화할 수 있음
    - 중간 계산 결과를 모두 보관할 수 있음
    - 역전파를 통해 ‘미분’을 효율적으로 계산할 수 있음
        - $\frac{\partial L}{\partial x}$ : x가 ‘아주 조금’ 증가하였을 때 ‘L’이 얼마나 증가하는가.

## 5.2. 연쇄 법칙

- 합성함수의 미분은 합성함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.
    - $y = f(g(x))$ 라고 할 때, $\frac{\partial{y}}{\partial{x}} = f’(g(x)) * g’(x)$ 가 성립한다.
    - $z=t^2$ 이고 $t=x+y$ 라 할 때,
    - $\frac{\partial{z}}{\partial{x}} = \frac{\partial{z}}{\partial{t}} * \frac{\partial{t}}{\partial{x}} = 2t * 1 = 2(x+y)$ * 1
        
        ![2](https://github.com/shinhee-rebecca/2023-deep-learning-study/assets/42907231/e3523c9c-4f93-4147-afbf-f0a838f32869)
        
- 계산 그래프 상의 역전파는 연쇄법칙의 원리에 따라 진행된다.
    
    > 노드로 들어온 입력 신호에다가 그 노드의 국소적 미분(편미분)을 곱한 후 다음 노드로 전달
    > 

## 5.3. 역전파

![3](https://github.com/shinhee-rebecca/2023-deep-learning-study/assets/42907231/b04d7fbd-486d-4608-922c-15b5d02ac409)

- 덧셈 노드의 역전파
    - $z = x +y$ 의 $x$, $y$ 각각에 대한 편미분
        - $\frac{\partial{z}} {\partial{x}} = 1$
        - $\frac{\partial{z}} {\partial{y}} = 1$
    - 덧셈 노드의 역전파는 1을 곱하기만 하고, 입력된 값을 그대로 다음 노드로 전달함
- 곱셉 노드의 역전파
    - $z = xy$ 의 $x$, $y$ 각각에 대한 편미분
        - $\frac{\partial{z}} {\partial{x}} = y$
        - $\frac{\partial{z}} {\partial{y}} = x$
    - 순전파 때의 입력 신호들을 서로 바꾼 값을 곱해서 다음 노드로 전달함

## 5.4. 단순한 계층 구현하기

### 곱셈 계층 구현

```python
class MulLayer:
	def __init__(self):
		self.x = None
		self.y = None
	
	def forward(self, x, y):
		self.x = x
		self.y = y
		# z = x * y
		out = x * y
		return out

	def backward(self, dout):
		# x 와 y를 서로 바꾼 값을 곱해준다.
		dx = dout * self.y 
		dy = dout * self.x
		return dx, dy
```

### 덧셈 계층 구현

```python
class AddLayer:
	def __init__(self):
		pass
	
	def forward(self, x, y):
		out = x + y
		return out
	
	def backward(self, dout):
		# 입력값을 그대로 전달한다.
		dx = dout * 1
		dy = dout * 1
		return dx, dy
```

```python

apple = 100
apple_num = 2
orange = 150
orange_num = 3
tax = 1.1

mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
mul_tax_layer = MulLayer()
add_apple_orange_layer = AddLayer()

# 순전파
apple_price = mul_apple_layer.forward(apple, apple_num)
orange_price = mul_orange_layer.forward(orange, orange_num)
all_price = add_apple_orange_layer.forward(apple_price, orange_price)
price = mul_tax_layer.forward(all_price, tax)

# 역전파
# 역전파의 입력: 순전파의 출력에 대한 미분
dprice = 1
dall_price, dtax = mul_tax_layer.backward(dprice)
dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)
dorange, dorange_num = mul_orange_layer.backward(dorange_price)
dapple, dapple_num = mul_apple_layer.backward(dapple_price)

print(price) # 715
print(dapple_num, dapple, dorange, dorange_num, dtax) # 110 2.2 3.3 165 650
```

## 활성화함수 계층 구현하기

### ReLU 계층 구현

- 순전파 때의 입력 x가 0보다 크다면 상류의 값을 그대로 하류로 보낸다.
- x가 0 이하이면 0을 보낸다.
    
    ![4](https://github.com/shinhee-rebecca/2023-deep-learning-study/assets/42907231/fa91315b-e13f-44b0-ab85-70a82fd88a0f)
    ![5](https://github.com/shinhee-rebecca/2023-deep-learning-study/assets/42907231/89afab81-5f1c-4ffa-b3a2-4a4ea7e5c4b3)
    

```python
class Relu:
	def __init__(self):
		self.mask = None

	def forward(self, x):
		self.mask = (x <= 0)
		out = x.copy()
		out[self.mask] = 0 # 0이하인 부분은 모두 0으로 처리
		return out

	def backward(self, dout):
		dout[self.mask] = 0 # 0이하인 부분은 모두 0을 흘려보낸다.
		dx = dout
		return dx
```

### Sigmoid 계층 구현

![6](https://github.com/shinhee-rebecca/2023-deep-learning-study/assets/42907231/4516534f-1f7a-425a-b741-7ace7fa9c882)

- $y = \frac{1}{1+exp(-x)}$
    - 1단계: $y = \frac{1}{x}$ 의 미분
        - $\frac{\partial{y}}{\partial{x}} = -\frac{1}{x^2} = -y^2$
    - 2단계: 덧셈의 미분
    - 3단계: $exp(x)$ 의 미분
        - $\frac{\partial{y}}{\partial{x}} = exp(x)$
            - $e^x$ 의 미분 = $e^x$
    - 4단계:
        - 곱셈의 미분
- 역전파의 최종 출력:
    - $\frac{\partial{L}}{\partial{y}} y^2 exp(-x)$
    - $= \frac{\partial{L}}{\partial{y}} \frac{1}{(1+exp(-x))^2} exp(-x)$
    - $= \frac{\partial{L}}{\partial{y}} \frac{1}{(1+exp(-x))} \frac{exp(-x)}{1+exp(-x)}$
    - $= \frac{\partial{L}}{\partial{y}} y(1-y)$
    - 순전파의 출력 y만으로 역전파를 계산할 수 있음.

```python
class Sigmoid:
	def __init__(self):
		self.out = None
	
	def forward(self, x):
		out = 1 / (1 + np.exp(-x))
		self.out = out
		return out

	def backward(self, dout):
		dx = dout * (1.0 - self.out) * self.out
		return dx
```

## 5.6. Affine/Softmax 계층 구현하기

![7](https://github.com/shinhee-rebecca/2023-deep-learning-study/assets/42907231/129e38b8-024e-45fd-980a-c9464898f5e6)


- $X * W + B= Y$ 의 역전파
    - $\frac{\partial{L}}{\partial{X}} = \frac{\partial{L}}{\partial{Y}} * W^T$
    - $\frac{\partial{L}}{\partial{W}} = X^T * \frac{\partial{L}}{\partial{Y}}$
    - 형상을 맞춰주어 내적이 가능하도록 해준다.

```python
class Affine:
	def __init__(self, w, b):
		self.W = W
		self.b = b
		self.x = None
		self.dW = None
		self.db = None

	def forward(self, x):
		self.x = x
		out = np.dot(x, self.W) + self.b #xW + b
		return out
	
	def backward(self, dout):
		dx = np.dot(dout, self.W.T) # 입력값을 서로 바꿔서 곱해줌.
		self.dW = np.dot(self.x.T, dout) # 입력값을 서로 바꿔서 곱해줌.
		self.db = np.sum(dout, axis=0) # 데이터를 단위로 한 축의 총합
		return dx
```

### Softmax-with-Loss 계층

- Softmax: $y_k = \frac{exp(a_k)}{\sum{exp(a_i)}}$
- Loss(cross-entropy): $L = -\sum{t_k\log{y_k}}$
- Softmax-with-Loss의 역전파: $y_k-t_k$

```python
class SoftmaxWithLoss:
	def __init__(self):
		self.loss = None # 손실
		self.y = None # 예측 (softmax의 출력)
		self.t = None # 정답 레이블 (one hot 벡터)

	def forward(self, x, t):
		self.t = t
		self.y = softmax(x)
		self.loss = cross_entropy_error(self.y, self.y)
		return self.loss

	def backward(self, dout=1):
		batch_size = self.t.shape[0]
		dx = (self.y - self.t) / batch_size # 역전파 시에는 batch size로 나누어서, 데이터 1개당 오차를 앞 계층으로 전파한다.
		return dx
```

## 5.7. 오차역전파법 구현하기

```python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from common.layers import *
from common.gradient import numerical_gradient
from collections import OrderedDict

class TwoLyerNet:
	def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
		# 가중치 초기화
		self.params = {}
		# np.random.randn: 평균=0, 표준편차=1의 가우시안 분포 난수 생성
		self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
		self.params['b1'] = np.zeros(hidden_size)
		self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
		self.params['b2'] = np.zeros(output_size)
		
		# 계층 생성
		self.layers = OrderDict()
		# 첫 번째 층
		self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
		# Relu
		self.layers['Relu1'] = Relu()
		# 두 번재 층
		self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
		# softmax with loss
		self.lastLayer = SoftmaxWithLoss()

	def predict(self, x):
		for layer in self.layers.values():
			x = layer.forward(x)
		return x

	def loss(self, x, t):
		y = self.predict(x)
		return self.lastLayer.forward(y, t)

	def accuracy(self, x, t):
		y = self.predict(x)
		y = np.argmax(y, axis=1)
		if t.ndim != 1: t = np.argmax(t, axis=1)
		accuracy = np.sum(y==t) / float(x.shape[0])
		return accuracy

	### numerical_gradient 구현 생략 ###
	def gradient(self, x, t):
		# 순전파
		self.loss(x, t)
		
		# 역전파
		dout = 1
		dout = self.lastLayer.backward(dout)

		layers = list(self.layers.values())
		layers.reverse()
		for layer in layers:
			dout = layer.backward(dout)

		# 결과 저장
		grads = {}
		grads['W1'] = self.layers['Affine1'].dW
		grads['b1'] = self.layers['Affine1'].db
		grads['W2'] = self.layers['Affine2'].dW
		grads['b2'] = self.layers['Affine2'].db
		
		return grads
```

### 오차역전파법을 사용한 학습 구현하기

```python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist
from tow_laeyr_net import TwoLayerNet

# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
network = TowLayerNet(input_size=784, hidden_size=50, output_size=10)

iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
		batch_mask = np.random.choice(train_size, batch_size)
		# 1. 미니배치 가져오기
		x_batch = x_train[batch_mask]
		t_batch = t_train[batch_mask]

		# 2. 기울기 산출 (오차역전파법)
		grad = network.gradient(x_batch, t_batch)
	
		# 3. 기울기만큼 파라미터 갱신
		for key in ('W1', 'b1', 'W2', 'b2'):
			network.params[key] -= learning_rate * grad[key]

		# loss 저장
		loss = network.loss(x_batch, t_batch)
		train_loss_list.append(loss)

		# 매 epoch 마다 성능 계산
		if i % iter_per_epoch == 0:
			train_acc = network.accuracy(x_train, t_train)
			test_acc = network.accuracy(x_test, t_test)
			train_acc_list.append(train_acc)
			test_acc_list.append(test_acc)
			print(train_acc, test_acc)
```
