# 자연어와 단어의 분산 표현

## 요약

- 시소러스를 이용해 유의어, 단어 간 유사도 측정
- 분포 가설에 기초한 단어 벡터화
- 동시발생행렬, PPMI 행렬, 차원 감소

## 시소러스

- 시소러스: 유의어 사전. 유의어들끼리 같은 그룹으로 분류
- WordNet 등이 존재
- 시소러스의 단점
    - 시대 변화에 대응하기 어려움
    - 시소러스를 만드는 데 필요한 인적 비용
    - 단어의 미묘한 차이를 표현할 수 없음

## 통계 기반 기법

- 말뭉치 전처리
    
    ```python
    def preprocess(text):
    	text = text.lower()
    	text = text.replace('.', ' .')
    	words = text.split(' ')
    	word_to_id = {} # 단어 -> 단어 id
    	id_to_word = {} # 단어 id -> 단어
    	for word in words:
    		if word not in word_to_id:
    			new_id = len(word_to_id)
    			word_to_id[word] = new_id
    			id_to_word[new_id] = word
    	
    	corpus = np.array([word_to_id[w] for w in words])
    	return corpus, word_to_id, id_to_word
    ```
    

### 단어의 분산 표현

- 단어의 의미를 벡터로 표현 → 분산 표현(distributioanl respresentation)
    - 단어의 분산 표현은 고정 길이의 밀집벡터(dense vector)로 표현함.

### 분포 가설

- 분포 가설: 단어의 의미는 주변 단어에 의해 형성된다. 단어 자체에는 의미ㅣ가 없고, 그 단어가 사용된 맥락이 의미를 형성한다.
    - 맥락: 특정 단어를 중심에 둔 그 주변 단어.
    - 윈도우 크기(window size): 맥락의 크기. 주변 단어를 몇 개나 포함할지를 결정

### 동시발생 행렬

- 모든 단어에 대해 동시 발생하는 단어를 표에 정리한 것.
    
    ```python
    def create_co_matrix(corpus, vocab_size, window_size=1):
    	corpus_size = len(corpus)
    	# 동시발생행렬 초기화
    	co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)
    	# 말뭉치의 모든 단어들에 대해 주변 단어 동시발생 빈도 업데이트
    	for idx, word_id in enumerate(corpus):
    		for i in range(1, window_size + 1):
    			left_idx = idx - i
    			right_idx = idx + i
    			# 윈도우 좌측 동시 발생 빈도 업데이트
    			if left_idx >= 0:
    				left_word_id = corpus[left_idx]
    				co_matrix[word_id, left_word_id] += 1
    			# 윈도우 우측 동시 발생 빈도 업데이트
    			if right_idx < corpus_size:
    				right_word_id = corpus[right_idx]
    				co_matrix[word_id, right_word_id] += 1
    	return co_matrix
    ```
    

### 벡터 간 유사도

- 코사인 유사도 (두 벡터가 가리키는 방향이 얼마나 비슷한가)
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dd399514-b6f8-4619-b4ad-db7b9a43643b/Untitled.png)
  
    - 벡터의 방향이 완전이 같다면 코사인 유사도는 1, 완전히 반대라면 -1
    
    ```python
    def cos_similarity(x, y, eps=1e-8):
    	nx = x / np.sqrt(np.sum(x**2) + eps) # 분모가 0이 되는 것을 방지
    	ny = y / np.sqrt(np.sum(y**2) + eps)
    	return np.dot(nx, ny)
    ```
    

## 통계 기반 기법 개선하기

### 상호정보량

- 동시발생 행렬의 단점
    - `the` 와 같이 출현빈도가 매우 높은 단어가 단어와 단어 간 의미 관계를 정확하게 표상하는 데 악영향을 끼칠 수 있음.
- 점별 상호정보량(Pointwise Mutual Information, PMI)
    
    $PMI(x, y) = log_2 \frac{P(x, y)}{P(x)P(y)}$
    
    - 값이 높을수록 관련성이 높다는 의미.
    - 단어가 단독으로 출현하는 횟수가 분모로 고려되므로, `the`와 같이 어디에서나 자주 출현하는 고빈도 단어의 영향력을 낮춤.
    - 그러나, 두 단어의 동시발생 횟수가 0이 되는 경우가 있으므로, 이를 방지하기 위헤 양의 상호정보량(Positive PMI, PPMI)를 사용함.
        - $PPMI(x, y) = max(0, PMI(x, y))$
            
            ```python
            def ppmi(C, eps=1e-8): # C: 동시 발생 행렬
            	M = np.zeros_like(C, dtype=np.float32) # ppmi 행렬 초기화
            	N = np.sum(C) # 동시 발생행렬의 전체 빈도 수
            	S = np.sum(C, axis=0) # 각 단어별 단독 출현 빈도
            	total = C.shape[0] * C.shape[1]
            	cnt = 0
            
            	for i in range(C.shape[0]):
            		for j in range(C.shape[1]):
            			pmi = np.log2(C[i, j] * N / S[j]*S[i]) + eps)
            			M[i, j] = max(0, pmi)
            	return M
            ```
            

### 차원 감소

- PPMI의 단점: 코퍼스의 어휘 수가 증가함에 따라 단어 벡터의 차원 수도 증가함. 또한 원소 대부분이 0임(희소벡터).
- 차원 감소: 벡터에서 중요한 정보는 최대한 유지하면서 차원을 줄이는 방법
- 특잇값분해(Singular Value Decomposition, SVD): 임의의 행렬을 세 행렬의 곱으로 분해
    - $X = USV^T$
        - $U$, $V$: 직교행렬
        - $S$: 대각행렬. 대각성분에는 특잇값이 큰 순서대로 나열되어 있음.
            - 특잇값: 해당 축의 중요도라고 간주할 수 있음.
                
                ![Untitled2](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0cdbf00d-a2f0-45c8-be99-ee9073964f86/Untitled.png)
              
                - $S$에서 특잇값이 작다면 중요도가 낮다는 뜻이므로, $U$에서 여분의 열벡터를 깎아내어 원래의 행렬을 근사할 수 있음.
                - $X$의 각 행에는 해당 단어 ID의 단어 벡터가 저장되어 있으며, 이것이 $U’$ 라는 차원 감소된 벡터로 표현됨.
    
    ```python
    text = 'You say goodbye and I say hello.'
    corpus, word_to_id, id_to_word = preprocess(text)
    vocab_size = len(id_to_word)
    C = create_co_matrix(corpus, vocab_size, window_size=1)
    W = ppmi(C)
    
    # SVD
    U, S, V = np.linalg.svd(W)
    
    print(C[0]) # 동시 발생 행렬
    print(W[0]) # ppmi 행렬
    print(U[0]) # SVD
    
    print(U[0, :2]) # 2차원으로 줄이기
    
    # [0 1 0 0 0 0 0]
    # [0.        1.8073549 0.        0.        0.        0.        0.       ]
    # [ 3.4094876e-01 -1.1102230e-16 -1.2051624e-01 -4.1633363e-16
    #  -9.3232495e-01 -1.1102230e-16 -2.4257469e-17]
    # [ 3.4094876e-01 -1.1102230e-16]
    ```
    
    - SVD를 통해 희소벡터를 밀집벡터로 변환할 수 있음.
    
    ```python
    # 각 단어를 2차원 벡터로 표현한 후 그래프 그리기
    for word, word_id in word_to_id.items():
    	plt.annotate(word, U[word_id, 0], U[word_id, 1]))
    plt.scatter(U[:0], U[:, 1], alpha=0.5)
    plt.show()
    ```
    
    ![Untitled3](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8f622238-1f12-4b76-831e-30dd2aad30a9/Untitled.png)
  
    PTB 데이터셋에 대한 SVD
    
    ```python
    # PTB dataset
    from common.util import most_similar, create_co_matrix, ppmi
    from dataset import ptb
    
    window_size = 2
    wordvec_size = 100
    
    corpus, word_to_id, id_to_word = ptb.load_data('train')
    vocab_size = len(word_to_id)
    # 동시발생빈도
    C = create_co_matrix(corpus, vocab_size, window_size)
    # PPMI
    W = ppmi(C)
    
    # SVD
    from sklearn.utils.extmath import randomized_svd
    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=42)
    word_vecs = U[:, :wordvec_size] # 정한 size만큼 차원 수 설정
    querys = ['you', 'year', 'car', 'toyota']
    for query in querys:
      most_similar(query, word_to_id, id_to_word, word_vecs, top=5)
    ```
    
    ```python
    [query] you
     i: 0.6776947379112244
     we: 0.6599374413490295
     bother: 0.5226162672042847
     do: 0.5223093628883362
     anybody: 0.5216633677482605
    
    [query] year
     month: 0.721190869808197
     quarter: 0.6038511991500854
     last: 0.6023232936859131
     february: 0.6016566157341003
     next: 0.5831355452537537
    
    [query] car
     auto: 0.6319928169250488
     luxury: 0.610816478729248
     cars: 0.5424427390098572
     corsica: 0.5238872766494751
     vehicle: 0.5197248458862305
    
    [query] toyota
     motor: 0.7367817759513855
     nissan: 0.7079684734344482
     motors: 0.6309563517570496
     lexus: 0.568770170211792
     infiniti: 0.5367591381072998
    ```
    
    Colab 실습
    
    [https://colab.research.google.com/drive/14vb97OzSZaw4Bm1wsDI3U6_c2oYX7s_g?usp=sharing](https://colab.research.google.com/drive/14vb97OzSZaw4Bm1wsDI3U6_c2oYX7s_g?usp=sharing)
